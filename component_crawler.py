#!/usr/bin/env python3
"""
ì›¹ì‚¬ì´íŠ¸ ì»´í¬ë„ŒíŠ¸ í¬ë¡¤ëŸ¬
íŠ¹ì • URLì˜ div class ëª…ì„ ì¶”ì¶œí•˜ì—¬ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í”„ë¡œê·¸ë¨
"""

import re
from datetime import datetime
from urllib.parse import urlparse
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager


class ComponentCrawler:
    """ì›¹ì‚¬ì´íŠ¸ ì»´í¬ë„ŒíŠ¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    def __init__(self, headless=True):
        """
        ì´ˆê¸°í™”
        Args:
            headless (bool): ë¸Œë¼ìš°ì €ë¥¼ ë³´ì´ì§€ ì•Šê²Œ ì‹¤í–‰í• ì§€ ì—¬ë¶€
        """
        self.headless = headless
        self.driver = None
        
    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        chrome_options = Options()
        
        if self.headless:
            chrome_options.add_argument('--headless')
        
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.driver.implicitly_wait(10)
        
    def extract_component_name(self, class_string):
        """
        class ë¬¸ìì—´ì—ì„œ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì´ë¦„ ì¶”ì¶œ
        Samsung ì»´í¬ë„ŒíŠ¸ ë„¤ì´ë° ê·œì¹™: AA##- ë˜ëŠ” AAA##- íŒ¨í„´
        ì˜ˆ: hd08-hero-kv-home, co76-feature-kv, co78-recommended-product-carousel
        
        Args:
            class_string (str): class ì†ì„± ë¬¸ìì—´
        Returns:
            str: ì£¼ìš” ì»´í¬ë„ŒíŠ¸ í´ë˜ìŠ¤ëª… (íŒ¨í„´ì— ë§ì§€ ì•Šìœ¼ë©´ None)
        """
        if not class_string:
            return None
        
        classes = class_string.split()
        
        # AA##- ë˜ëŠ” AAA##- íŒ¨í„´ì„ ë”°ë¥´ëŠ” ì»´í¬ë„ŒíŠ¸ë§Œ ì°¾ê¸°
        # ì˜ˆ: hd08-, co76-, srd19- ë“±
        component_pattern = re.compile(r'^[a-z]{2,3}\d{2}-')
        
        for cls in classes:
            if component_pattern.match(cls):
                return cls
        
        # íŒ¨í„´ì— ë§ëŠ” ì»´í¬ë„ŒíŠ¸ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜
        return None
    
    def extract_bem_component(self, class_name):
        """
        BEM íŒ¨í„´ì—ì„œ ì»´í¬ë„ŒíŠ¸ëª… ì¶”ì¶œ
        ì˜ˆ: nv16-country-selector__content-wrap -> nv16-country-selector
        
        Args:
            class_name (str): í´ë˜ìŠ¤ëª…
        Returns:
            str: ì»´í¬ë„ŒíŠ¸ëª… (BEMì˜ Block ë¶€ë¶„)
        """
        if not class_name:
            return class_name
        
        # __ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬ (BEMì˜ Block__Element íŒ¨í„´)
        if '__' in class_name:
            return class_name.split('__')[0]
        
        # -- ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬ (BEMì˜ Block--Modifier íŒ¨í„´)
        if '--' in class_name:
            return class_name.split('--')[0]
        
        return class_name
    
    def extract_site_code(self, url):
        """
        URLì—ì„œ Site Code ì¶”ì¶œ
        ì˜ˆ: https://www.samsung.com/uk/ -> UK
        
        Args:
            url (str): URL
        Returns:
            str: Site Code (ëŒ€ë¬¸ì)
        """
        try:
            parsed = urlparse(url)
            path_parts = [p for p in parsed.path.split('/') if p]
            if path_parts:
                return path_parts[0].upper()
            return "GLOBAL"
        except:
            return "UNKNOWN"
    
    def extract_page_type(self):
        """
        í˜ì´ì§€ íƒ€ì… ì¶”ì¶œ
        digitalData.page.pageInfo.pageTrack ê°’ì„ ì½ì–´ì˜´
        
        Returns:
            str: Page Type (ì˜ˆ: home, pdp, plp ë“±)
        """
        try:
            # digitalData.page.pageInfo.pageTrack ê°’ ê°€ì ¸ì˜¤ê¸°
            page_track = self.driver.execute_script("""
                try {
                    if (typeof digitalData !== 'undefined' && 
                        digitalData.page && 
                        digitalData.page.pageInfo && 
                        digitalData.page.pageInfo.pageTrack) {
                        return digitalData.page.pageInfo.pageTrack;
                    }
                } catch (e) {
                    return null;
                }
                return null;
            """)
            
            if page_track:
                # Camel í˜•íƒœë¡œ ë³€í™˜ (ì²« ê¸€ìë§Œ ëŒ€ë¬¸ì)
                return page_track.capitalize()
            
            return "Unknown"
            
        except Exception as e:
            print(f"   âš ï¸  Page Type ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return "Unknown"
    
    def crawl_divs(self, url):
        """
        URLì˜ div ìš”ì†Œë“¤ì˜ class ì¶”ì¶œ
        Args:
            url (str): í¬ë¡¤ë§í•  URL
        Returns:
            list: div ì •ë³´ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ” í¬ë¡¤ë§ ì‹œì‘: {url}")
        
        try:
            if not self.driver:
                self.setup_driver()
            
            # í˜ì´ì§€ ë¡œë“œ
            self.driver.get(url)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "div"))
            )
            
            # ì¶”ê°€ ë¡œë”© ì‹œê°„ (ë™ì  ì½˜í…ì¸ )
            import time
            time.sleep(3)
            
            # Site Code ì¶”ì¶œ
            site_code = self.extract_site_code(url)
            print(f"ğŸŒ Site Code: {site_code}")
            
            # Page Type ì¶”ì¶œ
            page_type = self.extract_page_type()
            print(f"ğŸ“„ Page Type: {page_type}")
            
            # ëª¨ë“  div ìš”ì†Œ ì°¾ê¸°
            divs = self.driver.find_elements(By.TAG_NAME, "div")
            
            print(f"âœ… ì´ {len(divs)}ê°œì˜ div ìš”ì†Œ ë°œê²¬")
            
            # ì»´í¬ë„ŒíŠ¸ë³„ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê¸° ìœ„í•œ ë”•ì…”ë„ˆë¦¬
            components_data = {}
            processed_classes = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì„¸íŠ¸ (í´ë˜ìŠ¤ëª… ê¸°ì¤€)
            
            for idx, div in enumerate(divs, 1):
                class_attr = div.get_attribute("class")
                
                if class_attr and class_attr.strip():
                    component_class = self.extract_component_name(class_attr)
                    
                    # ì»´í¬ë„ŒíŠ¸ íŒ¨í„´ì— ë§ëŠ” ê²ƒë§Œ ì¶”ì¶œ
                    if component_class:
                        # ì¤‘ë³µ ì²´í¬ (í´ë˜ìŠ¤ëª… ê¸°ì¤€)
                        if component_class in processed_classes:
                            continue
                        
                        processed_classes.add(component_class)
                        
                        # display ìŠ¤íƒ€ì¼ ì²´í¬
                        display_style = div.value_of_css_property("display")
                        is_displayed = display_style != "none"
                        
                        # BEM ì»´í¬ë„ŒíŠ¸ëª… ì¶”ì¶œ
                        component_name = self.extract_bem_component(component_class)
                        
                        # ì»´í¬ë„ŒíŠ¸ë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™”
                        if component_name not in components_data:
                            components_data[component_name] = {
                                'classes': [],
                                'display_y': 0,
                                'display_n': 0,
                                'all_classes': set()
                            }
                        
                        components_data[component_name]['classes'].append(component_class)
                        
                        if is_displayed:
                            components_data[component_name]['display_y'] += 1
                        else:
                            components_data[component_name]['display_n'] += 1
                        
                        # ì „ì²´ í´ë˜ìŠ¤ ëª©ë¡ ìˆ˜ì§‘
                        for cls in class_attr.split():
                            components_data[component_name]['all_classes'].add(cls)
            
            # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ì½”ë“œ ë‚´ ìˆœì„œëŒ€ë¡œ)
            results = []
            for idx, (component_name, data) in enumerate(components_data.items(), 1):
                results.append({
                    'ë²ˆí˜¸': idx,
                    'Site Code': site_code,
                    'Page Type': page_type,
                    'URL': url,
                    'ì»´í¬ë„ŒíŠ¸ëª…': component_name,
                    'ì „ì²´ í´ë˜ìŠ¤ ëª©ë¡': ', '.join(data['classes']),
                    'Display': f"Y:{data['display_y']} / N:{data['display_n']}"
                })
            
            total_classes = sum(len(data['classes']) for data in components_data.values())
            total_y = sum(data['display_y'] for data in components_data.values())
            total_n = sum(data['display_n'] for data in components_data.values())
            
            print(f"ğŸ“Š ì´ {len(results)}ê°œì˜ ê³ ìœ  ì»´í¬ë„ŒíŠ¸")
            print(f"   â”” ì´ í´ë˜ìŠ¤ ìˆ˜: {total_classes}ê°œ")
            print(f"   â”” Display Y: {total_y}ê°œ")
            print(f"   â”” Display N: {total_n}ê°œ")
            return results
            
        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ: {str(e)}")
            return []
    
    def save_to_excel(self, data, url):
        """
        ë°ì´í„°ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
        Args:
            data (list): ì €ì¥í•  ë°ì´í„°
            url (str): í¬ë¡¤ë§í•œ URL
        Returns:
            str: ì €ì¥ëœ íŒŒì¼ëª…
        """
        if not data:
            print("âš ï¸  ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # íŒŒì¼ëª… ìƒì„±
        domain = urlparse(url).netloc.replace('www.', '').replace('.', '_')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{domain}_components_{timestamp}.xlsx"
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(data)
        
        # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
        try:
            df.to_excel(filename, index=False, engine='openpyxl')
            print(f"âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            return filename
        except Exception as e:
            # Excel ì €ì¥ ì‹¤íŒ¨ì‹œ CSVë¡œ ì €ì¥
            csv_filename = filename.replace('.xlsx', '.csv')
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            print(f"âœ… CSV íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ: {csv_filename}")
            return csv_filename
    
    def close(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("ğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ•·ï¸  ì›¹ì‚¬ì´íŠ¸ ì»´í¬ë„ŒíŠ¸ í¬ë¡¤ëŸ¬")
    print("=" * 60)
    print()
    
    # URL ì…ë ¥
    url = input("í¬ë¡¤ë§í•  URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    
    if not url:
        print("âŒ URLì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # http/https í”„ë¡œí† ì½œ ì²´í¬
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    print()
    print(f"ğŸ¯ ëŒ€ìƒ URL: {url}")
    print()
    
    # í¬ë¡¤ëŸ¬ ì‹¤í–‰
    crawler = ComponentCrawler(headless=True)
    
    try:
        # div í¬ë¡¤ë§
        results = crawler.crawl_divs(url)
        
        if results:
            # ì—‘ì…€ ì €ì¥
            filename = crawler.save_to_excel(results, url)
            
            if filename:
                print()
                print("=" * 60)
                print(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
                print(f"ğŸ“ ì €ì¥ íŒŒì¼: {filename}")
                print(f"ğŸ“Š ì´ {len(results)}ê°œì˜ ì»´í¬ë„ŒíŠ¸ í´ë˜ìŠ¤ ì¶”ì¶œ")
                print("=" * 60)
        else:
            print("âš ï¸  ì¶”ì¶œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    except KeyboardInterrupt:
        print("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    finally:
        crawler.close()


if __name__ == "__main__":
    main()
